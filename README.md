LLM Prompt Injection Attack Sandbox
## Project Overview
This project provides a testing environment for simulating prompt injection and jailbreak attacks in large language models (LLMs) like ChatGPT and Claude. It includes built-in mitigation techniques and input sanitization strategies to secure LLM interactions.

## Features
Prompt Injection Simulation:
Tests various prompt injection techniques on LLMs to evaluate their robustness against malicious inputs.

Mitigation Techniques:
Incorporates input sanitization to prevent unauthorized or harmful commands during interactions.

Sandbox Environment:
Provides an isolated and controlled environment to analyze vulnerabilities safely without affecting production systems.

Support for Multiple LLMs:
Compatible with various LLMs including ChatGPT, Claude, and others, allowing cross-model vulnerability assessment.

## Technologies Used
Backend: Python, GPT APIs

Security Tools: Custom input sanitizers, jailbreak prevention

Deployment: Streamlit

Testing: Pytes